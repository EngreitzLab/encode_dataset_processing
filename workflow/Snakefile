from snakemake.utils import min_version
import os
import pandas as pd
min_version("7.0")
configfile: "config/config.yml"
conda: "mamba"

RESULTS_DIR = config["results_dir"]
BASE_DATASET_DIR = config["base_dataset_dir"]

DHS_HIC_DF = pd.read_csv(config["dhs_hic_metadata"], sep="\t")
DHS_ONLY_DF = pd.read_csv(config["dhs_only_metadata"], sep="\t")
FILTERED_PREDICTION_FILE_FORMAT_TEMPLATE = "threshold{threshold}{separator}{other_flags}"
THRESHOLDS = {
	"dhs_hic": "0.024",
	"dhs_only": "0.018"
}

# TODO: Fix hardcoding thresholds

def get_bai_files(df, df_type):
	bai_files = []
	for _, row in df.iterrows():
		dhs_encode_ids = row["DNase_ENCODE_ID"].split(", ")
		for dhs_encode_id in dhs_encode_ids:
			file = os.path.join(BASE_DATASET_DIR, df_type, row["Cluster"], f"{dhs_encode_id}_sorted.bam.bai")
			bai_files.append(file)
	return bai_files

def get_reformatted_predictions(df, df_type):
	formatted_pred_files = []
	for _, row in df.iterrows():
		full_file = os.path.join(RESULTS_DIR, df_type, row["Cluster"], "Predictions", f"abc_predictions_{row['Cluster']}_full_predictions.tsv.gz")
		thresh_file = os.path.join(RESULTS_DIR, df_type, row["Cluster"], "Predictions", f"abc_predictions_{row['Cluster']}_thresholded_predictions.tsv.gz")
		formatted_pred_files += [full_file, thresh_file]
	return formatted_pred_files

def get_bigInteract(df, df_type):
	files = []
	for _, row in df.iterrows():
		files.append(os.path.join(RESULTS_DIR, df_type, row["Cluster"], "Predictions", f"abc_predictions_{row['Cluster']}_thresholded_predictions.bigInteract"))
	return files

def get_bedpe(df, df_type):
	files = []
	for _, row in df.iterrows():
		files.append(os.path.join(RESULTS_DIR, df_type, row["Cluster"], "Predictions", f"abc_predictions_{row['Cluster']}_thresholded_predictions.bedpe.gz"))
	return files

def get_candidate_elements(df, df_type):
	files = []
	for _, row in df.iterrows():
		files.append(os.path.join(RESULTS_DIR, df_type, row["Cluster"], "Peaks", f"abc_predictions_{row['Cluster']}_candidate_elements.bed"))
	return files

rule all:
	input:
		# get_bai_files(DHS_HIC_DF, "dhs_hic"),
		# get_bai_files(DHS_ONLY_DF, "dhs_only"),
		# get_reformatted_predictions(DHS_HIC_DF, "dhs_hic"),
		# get_bedpe(DHS_HIC_DF, "dhs_hic"),
		get_candidate_elements(DHS_HIC_DF, "dhs_hic"),
		# get_bigInteract(DHS_HIC_DF, "dhs_hic"),
		# get_reformatted_predictions(DHS_ONLY_DF, "dhs_only"),
		# get_bedpe(DHS_ONLY_DF, "dhs_only"),
		get_candidate_elements(DHS_ONLY_DF, "dhs_only"),
		# get_bigInteract(DHS_ONLY_DF, "dhs_only")
	

rule sort_and_index_bam: 
	input:
		bam_file = os.path.join(BASE_DATASET_DIR, "{DF_TYPE}", "{CLUSTER}", "{DHS_ENCODE_ID}.bam")
	conda:
		"envs/env.yml"
	output: 
		sorted_file = os.path.join(BASE_DATASET_DIR, "{DF_TYPE}", "{CLUSTER}", "{DHS_ENCODE_ID}_sorted.bam"),
		bai_file = os.path.join(BASE_DATASET_DIR, "{DF_TYPE}", "{CLUSTER}", "{DHS_ENCODE_ID}_sorted.bam.bai")
	threads: 4
	resources:
		mem_mb=8000
	shell: 
		"""
		samtools sort -@ 4 -m 1G {input.bam_file} -o {output.sorted_file}
		samtools index -@ 4 {output.sorted_file}
		"""

rule format_file:
	input: 
		pred = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "EnhancerPredictionsAllPutative.tsv.gz"),
		pred_thresholded = lambda wildcard: os.path.join(RESULTS_DIR, wildcard.DF_TYPE, wildcard.CLUSTER, "Predictions", f"EnhancerPredictionsFull_threshold{THRESHOLDS[wildcard.DF_TYPE]}_self_promoter.tsv"), 
	params:
		metadata = "metadata/{DF_TYPE}_raw.tsv"
	output: 
		pred = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_full_predictions.tsv.gz"),
		pred_thresholded = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_thresholded_predictions.tsv.gz"), 
	conda: 
		"envs/env.yml"
	resources:
		mem = "16G"
	shell:
		"""
		python workflow/scripts/reformat_abc_predictions.py --predictions {input.pred} --metadata {params.metadata} --output {output.pred}
		python workflow/scripts/reformat_abc_predictions.py --predictions {input.pred_thresholded} --metadata {params.metadata} --output {output.pred_thresholded}
		"""

rule create_bigInteract:
	input: 
		pred = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_full_predictions.tsv.gz"),
		pred_thresholded = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_thresholded_predictions.tsv.gz"), 
		chrs = "resources/GRCh38_EBV.chrom.sizes.tsv",
		int_as = "resources/interact.as"
	output: 
		Int = temp(os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_thresholded_predictions.interact")),
		bigInt = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_thresholded_predictions.bigInteract")
	conda: 
		"envs/big_interact.yml"
	resources:
		mem = "16G"
	script:
		"scripts/create_bigInteract.R"

rule create_bedpe:
	input: 
		bedpe = lambda wildcard: os.path.join(RESULTS_DIR, wildcard.DF_TYPE, wildcard.CLUSTER, "Predictions", f"EnhancerPredictionsFull_threshold{THRESHOLDS[wildcard.DF_TYPE]}_self_promoter.bedpe.gz"), 
	output: 
		bedpe = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Predictions", "abc_predictions_{CLUSTER}_thresholded_predictions.bedpe.gz")
	resources:
		mem = "2G"
	shell:
		"""
		ln -rs {input.bedpe} {output.bedpe}
		"""

rule create_candidate_elements:
	input: 
		candidate_elements = lambda wildcard: os.path.join(RESULTS_DIR, wildcard.DF_TYPE, wildcard.CLUSTER, "Peaks", "macs2_peaks.narrowPeak.sorted.candidateRegions.bed"), 
	output: 
		candidate_elements = os.path.join(RESULTS_DIR, "{DF_TYPE}", "{CLUSTER}", "Peaks", "abc_predictions_{CLUSTER}_candidate_elements.bed")
	resources:
		mem = "2G"
	shell:
		"""
		ln -rs {input.candidate_elements} {output.candidate_elements}
		"""